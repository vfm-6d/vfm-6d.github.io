<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>VFM</title>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>
<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-2 publication-title">Vision Foundation Model Enables Generalizable Object Pose Estimation</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://ck-kai.github.io/" target="_blank">Kai Chen</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="" target="_blank">Yiyao Ma</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="https://xingyu-lin.github.io" target="_blank">Xingyu Lin</a><sup>2</sup>,
            </span>
            <span class="author-block">
              <a href="https://stepjam.github.io/" target="_blank">Stephen James</a><sup>3</sup>,
            </span>
            <span class="author-block">
              <a href="https://scholar.google.com/citations?user=HVofSgQAAAAJ&hl=zh-CN" target="_blank">Jianshu Zhou</a><sup>1</sup>,
            </span><br>
            <span class="author-block">
              <a href="http://ri.cuhk.edu.hk/yhliu" target="_blank">Yun-Hui Liu</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="https://people.eecs.berkeley.edu/~pabbeel/" target="_blank">Pieter Abbeel</a><sup>2</sup>,
            </span>
            <span class="author-block">
              <a href="https://www.cse.cuhk.edu.hk/~qdou/" target="_blank">Qi Dou</a><sup>1</sup>
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup><a href = "https://www.cuhk.edu.hk/english/index.html" target="_blank"> The Chinese University of Hong Kong </a>,</span>
            <span class="author-block"><sup>2</sup><a href = "https://www.berkeley.edu/" target="_blank"> University of California, Berkeley </a> ,</span>
            <span class="author-block"><sup>3</sup><a href = "https://careers.dyson.com/en-gb/" target="_blank"> Dyson Robot Learning Lab </a></span>
          </div>


          <div class="column has-text-centered">
            <div class="publication-links">
              <span class="link-block">
                <a href=""
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <span class="link-block">
                <a href=""
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code (coming soon)</span>
                  </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-left">
      <div class="column">
        <img src="./static/images/teaser.png" alt="teaser" class="teaser-image"><br>
        <p class="has-text-centered">
          VFM-6D uses cost-effective synthetic data to adapt robust object representations from pre-trained vision foundation models to the task of object pose estimation. VFM-6D supports both instance-level unseen object pose estimation and category-level pose estimation for novel object categories.
        </p>
      </div>
    </div>
  </div>
  <br>
</section>

<section class="hero is-light">
  <br>
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Object pose estimation plays a crucial role in robotic manipulation, however, its practical applicability still suffers from limited generalizability. This paper addresses the challenge of generalizable object pose estimation, particularly focusing on category-level object pose estimation for unseen object categories. Current methods either require impractical instance-level training or are confined to predefined categories, limiting their applicability. We propose VFM-6D, a novel framework that explores harnessing existing vision and language models, to elaborate object pose estimation into two stages: category-level object viewpoint estimation and object coordinate map estimation. Based on the two-stage framework, we introduce a 2D-to-3D feature lifting module and a shape-matching module, both of which leverage pre-trained vision foundation models to improve object representation and matching accuracy. VFM-6D is trained on cost-effective synthetic data and exhibits superior generalization capabilities. It can be applied to both instance-level unseen object pose estimation and category-level object pose estimation for novel categories. Evaluations on benchmark datasets demonstrate the effectiveness and versatility of VFM-6D in various real-world scenarios.
          </p><br>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-left">
      <div class="column">
        <h2 class="title is-4">Method Overview</h2>
      </div>
    </div>
    <div class="columns is-centered has-text-justified">
      <div class="column is-12">
        <p>
          VFM-6D elaborates object pose estimation into two stages: foundation model based object viewpoint estimation and reference-based object coordinate map estimation.
          First, a 2D-to-3D foundation feature lifting module is developed to adapt 2D pre-trained features to 3D view-aware object representations for precise query-reference matching and object viewpoint estimation.
          Building on the estimated object viewpoint and the matched reference image, we further introduce a foundation-feature-based object 3D shape representation module.          
          It enhances robust shape matching and coordinate map estimation across a variety of objects for generalizable object pose estimation.
        </p>
      </div>
    </div>
    <div class="columns is-centered has-text-justified">
      <div class="column is-12">
        <img src="./static/images/framework.png" alt="teaser" class="teaser-image"><br>
      </div>
    </div>
    <div class="columns is-centered has-text-justified">
      <div class="column is-12">
        <p>
          We evaluate VFM-6D on Wild6D benchmark dataset comprised of 162 different object instances in 5 table-top categories, and on CO3D dataset comprised of 200 different object instances in 20 categories.
          Moreover, we also evaluate VFM-6D on instance-level LINEMOD benchmark dataset, which consists of 13 untextured object instances.
          Our experiments demonstrate the superior generalization capability of VFM-6D.          
        </p>
        <br>
      </div>
    </div>
    <div class="columns is-multiline">
      <div class="column is-12">
        <h2 class="title is-5">Object Pose Estimation Results Visualization</h2>
      </div>
      <div class="column is-12 has-text-left">
        <p>
          Use the tabs and the dropdown menu to select the object category / instance name in the corresponding benchmark. 
          For category-level evaluation, we show the used shape template on the left side of the image. The first row shows the predicted object coordinate map, 
          and the second row visualizes the predicted object pose.
          For instance-level evaluation, we use the corresponding instance-level object CAD model during experiments.
        </p>
      </div>
    </div>
    <div class="tabs is-toggle is-fullwidth" id="policyTaskTabs">
      <ul>
        <li class="tablinks" id="clickDefault" onclick="switchTab(event, 'wild6d')"><a><b>Wild6D</b></a></li>
        <li class="tablinks" onclick="switchTab(event, 'co3d')"><a><b>CO3D</b></a></li>
        <li class="tablinks" onclick="switchTab(event, 'linemod')"><a><b>LINEMOD</b></a></li>
      </ul>
    </div>
    <div class="columns is-centered has-text-centered">
      <div class="column is-12">
        <div class="field">
          <div class="control">
            <div class="select">
              <select id="dropdown2" onchange="changeImage()">
              </select>
            </div>
          </div>
        </div>
      </div>
    </div>
    <div class="columns is-centered has-text-justified">
      <div class="column is-12">
        <img id="displayed-image" src="" alt="Selected Image">
      </div>
    </div>
    <br>
    <div class="columns is-centered is-multiline">
      <div class="column is-12">
        <h2 class="title is-5">VFM-6D in Open-World Scenarios</h2>
      </div>
      <div class="column is-12 has-text-left">
        <p>
          In open-world scenarios, it is possible that the shape template corresponding
          to the category of the observed object is not collected in advance. 
          By integrating VFM-6D with GPT-4V and text-to-3D model, we demonstrate the robustness and generalization capability of VFM-6D in handling open-world scenarios involving novel object categories.
          In our evaluation, we exploit the text-to-3D generation function provided by Meshy.
        </p>
      </div>
    </div>
    <div class="columns is-centered is-multiline">
      <div class="column is-12 low-marg">
        <p>
          <strong>Single-category static open-world scenes</strong>:
        </p>
      </div>
      <div class="columns is-centered has-text-justified">
        <div class="column is-10">
          <img src="./static/images/open-world1.png" alt="teaser" class="teaser-image"><br>
        </div>
      </div>
      <div class="column is-12 low-marg">
        <p>
          <strong>Multi-category dynamic manipulation scenes</strong>:
        </p>
      </div>
      <div class="columns is-centered has-text-justified">
        <div class="column is-10">
          <img src="./static/images/open-world2.png" alt="teaser" class="teaser-image"><br>
        </div>
      </div>
    </div>
    <!-- <br>
    <div class="columns is-centered is-multiline">
      <div class="column is-12">
        <h2 class="title is-5">VFM-6D with Different Vision Foundation Models</h2>
      </div>
      <div class="column is-12 has-text-left">
        <p>
          To better understand the advantages of ATM's track subgoals, we compare them qualitatively to image subgoals generated by <b>UniPi (left)</b>. To decouple the advantages of open-loop and closed-loop video generation, we additionally instantiate <b>UniPi-Replan (right)</b>, which proposes new image subgoals every 8 actions.
        </p>
        <br>
        <p>
          Qualitatively, UniPi suffers from motor control failure caused by a lack of fine-grained details in image subgoals. UniPi-Replan additionally experiences failures in image generation, generating noisy images when out of distribution, or generating images that correspond to a different task.
        </p>
      </div>
    </div>
    <div class="container affordance-viz">
      <div class="carousel">
        <div class="item">
          <div class="item-container">
            <p>
              <strong>Pick up the alphabet soup and place it in the basket</strong>: UniPi-Replan fails to pick up the soup. It is difficult to determine whether the generated image subgoals reach for the soup can in the back, or the carton in the front.
            </p>
          </div>
          <div class="item-container">
            <video class="temp" autoplay muted loop playsinline width="100%">
              <source src="./static/videos/new_comp//ATM/obj-0-trimmed.mp4"
                      type="video/mp4">
            </video>
            <video class="comp-vid" autoplay muted loop playsinline width="100%">
              <source src="./static/videos/new_comp/unipi/obj-0-succ.mp4"
                      type="video/mp4">
            </video>
            <video class="comp-vid" autoplay muted loop playsinline width="100%">
              <source src="./static/videos/new_comp//unipi-replan/obj-0-generation.mp4"
                      type="video/mp4">
            </video>
          </div>
          <div class="item-container">
            <div class="comp-item-label temp">
              <p><strong>ATM</strong></p>
            </div>
            <div class="comp-item-label">
              <p><strong>UniPi</strong></p>
            </div>
            <div class="comp-item-label">
              <p><strong>UniPi-Replan</strong></p>
            </div>
          </div>
        </div>
        <div class="item">
          <div class="item-container">
            <p>
              <strong>Pick up the bbq sauce and place it in the basket</strong>: UniPi fails to pick up the bbq sauce, as image subgoals lack finer details relevant to motor control.
            </p>
          </div>
          <div class="item-container">
            <video class="temp" autoplay muted loop playsinline width="100%">
              <source src="./static/videos/new_comp//ATM/obj-1-trimmed.mp4"
                      type="video/mp4">
            </video>
            <video class="comp-vid" autoplay muted loop playsinline width="100%">
              <source src="./static/videos/new_comp//unipi/obj-1-motor.mp4"
                      type="video/mp4">
            </video>
            <video class="comp-vid" autoplay muted loop playsinline width="100%">
              <source src="./static/videos/new_comp//unipi-replan/obj-1-succ.mp4"
                      type="video/mp4">
            </video>
          </div>
          <div class="item-container">
            <div class="comp-item-label temp">
              <p><strong>ATM</strong></p>
            </div>
            <div class="comp-item-label">
              <p><strong>UniPi</strong></p>
            </div>
            <div class="comp-item-label">
              <p><strong>UniPi-Replan</strong></p>
            </div>
          </div>
        </div>
        <div class="item">
          <div class="item-container">
            <p>
              <strong>Open the middle drawer of the cabinet</strong>: UniPi-Replan's diffusion model generates subgoals corresponding to a different task, indicating the increased difficulty of closed-loop video generation.
            </p>
          </div>
          <div class="item-container">
            <video class="temp" autoplay muted loop playsinline width="100%">
              <source src="./static/videos/new_comp//ATM/goal-0-trimmed.mp4"
                      type="video/mp4">
            </video>
            <video class="comp-vid" autoplay muted loop playsinline width="100%">
              <source src="./static/videos/new_comp//unipi/goal-0-succ.mp4"
                      type="video/mp4">
            </video>
            <video class="comp-vid" autoplay muted loop playsinline width="100%">
              <source src="./static/videos/new_comp//unipi-replan/goal-0-wrong-task.mp4"
                      type="video/mp4">
            </video>
          </div>
          <div class="item-container">
            <div class="comp-item-label temp">
              <p><strong>ATM</strong></p>
            </div>
            <div class="comp-item-label">
              <p><strong>UniPi</strong></p>
            </div>
            <div class="comp-item-label">
              <p><strong>UniPi-Replan</strong></p>
            </div>
          </div>
        </div>
        <div class="item">
          <div class="item-container">
            <p>
              <strong>Open the top drawer of the cabinet</strong>: Both UniPi and UniPi-Replan experience motor control failure, as it is difficult to tell when to close the gripper from noisy image subgoals.
            </p>
          </div>
          <div class="item-container">
            <video class="temp" autoplay muted loop playsinline width="100%">
              <source src="./static/videos/new_comp//ATM/goal-1-trimmed.mp4"
                      type="video/mp4">
            </video>
            <video class="comp-vid" autoplay muted loop playsinline width="100%">
              <source src="./static/videos/new_comp//unipi/goal-1-motor.mp4"
                      type="video/mp4">
            </video>
            <video class="comp-vid" autoplay muted loop playsinline width="100%">
              <source src="./static/videos/new_comp//unipi-replan/goal-1-motor.mp4"
                      type="video/mp4">
            </video>
          </div>
          <div class="item-container">
            <div class="comp-item-label temp">
              <p><strong>ATM</strong></p>
            </div>
            <div class="comp-item-label">
              <p><strong>UniPi</strong></p>
            </div>
            <div class="comp-item-label">
              <p><strong>UniPi-Replan</strong></p>
            </div>
          </div>
        </div>
      </div>
    </div> -->
  </div>
</section>
<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="titile">BibTeX</h2>
    <pre><code>@misc{kai2024vfm,
      title={Vision Foundation Model Enables Generalizable Object Pose Estimation},
      author={Kai Chen and Yiyao Ma and Xingyu Lin and Stephen James and Jianshu Zhou and Yun-Hui Liu and Pieter Abbeel and Qi Dou},
      booktitle={Neural Information Processing Systems (NeurIPS)},
      month={December},
      year={2024}
}</code></pre>
  </div>
</section>

<footer class="custom_footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            Template borrowed from <a href="https://xingyu-lin.github.io/atm/">ATM project</a> and <a href="https://github.com/nerfies/nerfies.github.io">Nerfies</a>.
          </p><br>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
